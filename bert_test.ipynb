{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 6821, 3221,  671,  702, 3844, 6407, 3152, 3667,  117, 3844, 6407,\n",
      "         4294, 3654, 2099, 5016,  102],\n",
      "        [ 101, 2769,  812, 1086, 1091,  671,  702, 3844, 6407, 2099, 5016,  706,\n",
      "          102,    0,    0,    0,    0],\n",
      "        [ 101,  809,  912, 6822, 6121,  671,  763, 7270, 2428,  679, 1398, 4638,\n",
      "         3173, 4638, 3844, 6407,  102],\n",
      "        [ 101,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 101,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[CLS] 这 是 一 个 测 试 文 段, 测 试 特 殊 字 符 [SEP]'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = [\n",
    "    '这是一个测试文段,测试特殊字符',\n",
    "    '我们再写一个测试字符串',\n",
    "    '以便进行一些长度不同的新的测试',\n",
    "    '',\n",
    "    ''\n",
    "    ]\n",
    "\n",
    "doc2 = [\n",
    "    '我们写了第二个测试文档',\n",
    "    '以便处理文本长度不一致的问题'\n",
    "]\n",
    "string = [doc1, doc2]\n",
    "token_str = tokenizer(\n",
    "    #string,\n",
    "    doc1,\n",
    "    padding = True, \n",
    "    return_tensors = 'pt',\n",
    "    max_length=30,\n",
    "    truncation=True\n",
    "    )\n",
    "for k in token_str.keys():\n",
    "    print(k, token_str[k])\n",
    "tokenizer.decode(token_str['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 6821, 3221,  671,  702, 3844, 6407, 3152, 3667,  117, 3844, 6407,\n",
      "         4294, 3654, 2099, 5016,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2769,  812, 1086, 1091,  671,  702, 3844, 6407, 2099, 5016,  706,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  809,  912, 6822, 6121,  671,  763, 7270, 2428,  679, 1398, 4638,\n",
      "         3173, 4638, 3844, 6407,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros(3, 30).type(torch.long)\n",
    "t[0:3, 0:17] = token_str['input_ids']\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "bert_output = bert(\n",
    "    input_ids = token_str['input_ids'],\n",
    "    attention_mask = token_str['attention_mask'],\n",
    "    #token_type_ids = token_str['token_type_ids']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [2, 3].  Tensor sizes: [2, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ddbd99038d31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mptp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# use index notation to prevent duplicate references to the tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mout_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mout_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [2, 3].  Tensor sizes: [2, 4]"
     ]
    }
   ],
   "source": [
    "t1 = torch.arange(9).view(-1,3)\n",
    "t2 = torch.arange(8).view(-1,4)\n",
    "l = [3,2]\n",
    "tp = pad_sequence([t1,t2], batch_first=True)\n",
    "ptp = pack_padded_sequence(tp, l, batch_first=True)\n",
    "print(t1,t2,tp, sep='\\n')\n",
    "print(ptp)\n",
    "bert(ptp.data).pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor([[1,2,3,4,5,6,7,8,0,0,0,0]]) #.view(3,4)\n",
    "mask = (torch.tensor([[1,2,3,4,5,6,7,8,0,0,0,0]]) > 0).type(torch.int)\n",
    "#test_mask = torch.zeros(3,4)\n",
    "#print(test_input, test_mask)\n",
    "t1 = bert(input_ids = test_input).pooler_output\n",
    "\n",
    "test_input = torch.tensor([[1,2,3,4,5,6,7,8,0,0]])#.view(3,4)\n",
    "mask = (torch.tensor([[1,2,3,4,5,6,7,8,0,0]]) > 0).type(torch.int)\n",
    "t2 = bert(input_ids = test_input).pooler_output\n",
    "print(torch.allclose(t1,t2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "mask = (torch.tensor([[1,2,3,4,5,6,7,8,0,0,0,0]]) > 0).type(torch.int)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101,   0,   0, 102],\n",
      "        [101,   0,   0, 102],\n",
      "        [101,   0,   0, 102]]) tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[ 0.9827,  0.9999,  0.9666,  ..., -0.9996, -0.9377,  0.7818],\n",
      "        [ 0.9827,  0.9999,  0.9666,  ..., -0.9996, -0.9377,  0.7818],\n",
      "        [ 0.9827,  0.9999,  0.9666,  ..., -0.9996, -0.9377,  0.7818]],\n",
      "       grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.zeros(3,4).type(torch.long)\n",
    "test_input[:,0] = 101\n",
    "test_input[:,-1] = 102\n",
    "test_mask = torch.zeros(3,4)\n",
    "print(test_input, test_mask)\n",
    "print(bert(input_ids = test_input).pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.9998,  0.9998,  0.9983,  ..., -0.9996, -0.9990, -0.5405],\n        [ 0.9987,  0.9998,  0.9990,  ..., -0.9975, -0.9903, -0.7868],\n        [ 0.9994,  1.0000,  0.9030,  ..., -0.9949, -0.9962,  0.8239]],\n       grad_fn=<TanhBackward>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.]]), batch_sizes=tensor([2, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.arange(0,16).view(-1,4)\n",
    "t1 = torch.cat([t1, torch.zeros(1,4)], dim = 0)\n",
    "\n",
    "t2 = torch.arange(0,12).view(-1,4)\n",
    "t2 = torch.cat([t2, torch.zeros(2,4)], dim = 0)\n",
    "\n",
    "t = torch.stack([t1,t2], dim=0)\n",
    "l = [4,3]\n",
    "ps = pack_padded_sequence(t, l, batch_first=True)\n",
    "\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state \t tensor([[[-0.6608,  0.4741,  0.6114,  ...,  0.2885,  0.0974,  0.1135],\n",
      "         [-0.3507,  0.6145,  1.2518,  ..., -0.2922, -0.5560,  0.0368],\n",
      "         [-0.2561, -0.0083,  1.3459,  ..., -0.1950,  0.3284,  0.1385],\n",
      "         [-0.7835,  0.7552,  0.0676,  ...,  0.2537,  0.1146, -0.1255],\n",
      "         [-0.1400,  0.4187,  1.0834,  ..., -0.0825, -0.4506,  0.5955]],\n",
      "\n",
      "        [[-0.0362,  0.7508, -0.3406,  ...,  0.2986, -0.3089, -0.1495],\n",
      "         [-0.0112, -0.1314, -0.3386,  ..., -0.7059, -0.5913,  0.0548],\n",
      "         [ 0.3185, -0.6601, -0.1627,  ..., -0.0639,  0.2416,  0.0647],\n",
      "         [ 0.2119,  0.9714, -0.1111,  ..., -0.3018, -0.5880, -0.1860],\n",
      "         [-0.6646,  0.1629, -0.0872,  ..., -0.3346, -0.0014, -0.0975]],\n",
      "\n",
      "        [[ 0.1191,  0.5861,  0.6062,  ...,  0.5815,  0.4746, -0.2843],\n",
      "         [ 0.2881,  0.2342,  1.1484,  ..., -0.7969, -0.3408,  0.1214],\n",
      "         [ 0.2756,  0.3449,  0.7496,  ...,  0.5526,  0.5725,  0.1504],\n",
      "         [ 0.2385,  0.6318,  0.2960,  ..., -0.2246,  0.5614, -0.6867],\n",
      "         [-0.1021,  0.1906,  0.5063,  ..., -0.2329, -0.0028, -0.1543]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "pooler_output \t tensor([[ 0.9976,  1.0000,  0.8924,  ..., -0.9991, -0.9479,  0.8802],\n",
      "        [ 0.9998,  1.0000,  0.9998,  ..., -0.9988, -0.9984,  0.1422],\n",
      "        [ 0.9999,  1.0000,  0.9725,  ..., -0.9872, -0.9935,  0.9663]],\n",
      "       grad_fn=<TanhBackward>)\n",
      "hidden_states \t None\n",
      "past_key_values \t None\n",
      "attentions \t None\n",
      "cross_attentions \t None\n"
     ]
    }
   ],
   "source": [
    "for k in bert_output.__dict__.keys():\n",
    "    print(k,'\\t', bert_output.__dict__[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 768])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs = bert_output['pooler_output']\n",
    "lhs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32])\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 768\n",
    "HIDDEN_SIZE = 16\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "bi_lstm = nn.LSTM(\n",
    "    input_size = INPUT_SIZE,\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    batch_first = True,\n",
    "    bidirectional = True\n",
    ")\n",
    "output,(_,_) = bi_lstm(\n",
    "    bert_output['pooler_output'].reshape(1,3,768)\n",
    ")\n",
    "print(output[:,-1,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_para():\n",
    "    p = './data/doc_quality_data_train.json'\n",
    "    para_list = []\n",
    "    with open(p,'r') as f:\n",
    "        for i in range(3):\n",
    "            l = json.loads(f.readline())\n",
    "            para_list.append(l['body'])\n",
    "    return para_list\n",
    "para_list = get_sample_para()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['篮球——CBA第四阶段：辽宁本钢迎战吉林九台农场银行',\n '新华社照片，诸暨（浙江），2021年3月26日',\n '（体育）（13）篮球——CBA第四阶段：辽宁本钢迎战吉林九台农场银行',\n '3月26日，辽宁本钢队主教练杨鸣在比赛中指挥。',\n '当日，在浙江诸暨举行的2020-2021赛季中国男子篮球职业联赛（CBA）第四阶段第48轮比赛中，辽宁本钢队对阵吉林九台农商银行队。',\n '新华社记者',\n '孟永民',\n '摄']"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _cut_doc(para):\n",
    "    '''\n",
    "    remain to modify: \n",
    "    1. maybe we can delete all the punctuations\n",
    "    '''\n",
    "    para = re.sub('\\s*([。！？;\\?])([^”’])\\s*', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('\\s*(\\.{6})([^”’])\\s*', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('\\s*(\\…{2})([^”’])\\s*', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('\\s*([。！？\\?][”’])([^，。！？\\?])\\s*', r'\\1\\n\\2', para)\n",
    "    para = re.sub(r'\\s+', r'\\n', para)\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    return para.split(\"\\n\")\n",
    "\n",
    "_cut_doc(para_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cut_docs(docs):\n",
    "    '''\n",
    "    docs: list of docs\n",
    "    '''\n",
    "    return [_cut_doc(d) for d in docs]\n",
    "docs_seq = _cut_docs(para_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 44, [[26, 23, 34, 23, 66, 5, 3, 1], [43, 38, 29, 28, 25, 13, 15, 48, 26, 33, 38, 32, 30, 36, 37, 47, 39, 18, 19, 9, 25, 35, 29, 68, 27, 37, 20, 21, 34, 20, 56, 23, 40, 17, 51, 52, 34, 16, 62, 28, 23, 55, 31, 1], [10, 3, 55]], [8, 44, 3])\n"
     ]
    }
   ],
   "source": [
    "def get_lengths(docs_sequences):\n",
    "    doc_lengths = [len(d) for d in docs_sequences]\n",
    "    sent_lengths = []\n",
    "    for d in docs_sequences:\n",
    "        sent_lengths.append([len(y) for y in d])\n",
    "    max_sent_length = max([max(y) for y in sent_lengths])\n",
    "    max_doc_length = max(doc_lengths)\n",
    "\n",
    "    return max_sent_length, max_doc_length, sent_lengths, doc_lengths\n",
    "    \n",
    "print(get_lengths(docs_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_preprocess(docs, bertTokenizer, bert_max_length = 512):\n",
    "    #docs_seq = _cut_docs(docs)\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    token_type_ids = []\n",
    "    for d in docs_seq:\n",
    "        tk = bertTokenizer(d, return_tensors = 'pt', padding = True)\n",
    "        input_ids.append(tk['input_ids'])\n",
    "        attention_mask.append(tk['attention_mask'])\n",
    "        token_type_ids.append(tk['token_type_ids'])\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "input_ids, attention_mask, token_type_ids = bert_preprocess(string, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 5 # 词典大小V=5\n",
    "words = range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 2, 3]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2,3], [0,0,1,2,4], [1,0,0,4], [4,4,2]]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 首先按照序列长度由大到小排序\n",
    "data = sorted(data, key=len, reverse=True)\n",
    "\n",
    "lengths = [len(ins) for ins in data] # 每条训练样本的序列长度\n",
    "T = len(data[0]) # 最大的序列长度\n",
    "B = len(data)  # batch_size\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot_encoding(input, T, V):\n",
    "    data = np.zeros((T, V)) # (T, V)\n",
    "    data[np.array(range(len(input))), np.array(input)] = 1\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [convert_one_hot_encoding(seq, T, V) for seq in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 1.]]),\n array([[0., 1., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0.]]),\n array([[0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]]),\n array([[0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}